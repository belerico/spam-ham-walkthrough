{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXs-riOaHPKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if you want to use google's colab, \n",
        "# you can upload compressed files to colab and uncompress them like below\n",
        "# You can download them: http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html\n",
        "!tar -zxvf enron1.tar.gz\n",
        "!tar -zxvf enron2.tar.gz\n",
        "!tar -zxvf enron3.tar.gz\n",
        "!tar -zxvf enron4.tar.gz\n",
        "!tar -zxvf enron5.tar.gz\n",
        "!tar -zxvf enron6.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG9yAN2qHqv2",
        "colab_type": "code",
        "outputId": "543bdb14-6fc9-44d3-8ed1-fad82e92b085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# To use file_reader.py in colab, upload it to colab.\n",
        "from file_reader import File_reader\n",
        "\n",
        "fr = File_reader()\n",
        "\n",
        "data, label = fr.load_ham_and_spam(ham_paths = \"default\", spam_paths = \"default\", max = 3000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham length  3000\n",
            "spam length  3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owAIBuR0SYgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "vocabs = [vocab for seq in data for vocab in seq.split()]\n",
        "# a = [  word for seq in [\"a d\",\"b d\",\"c d\"] for word in seq.split() ]\n",
        "# ['a', 'd', 'b', 'd', 'c', 'd']\n",
        "\n",
        "vocab_count = Counter(vocabs)\n",
        "# Count words in the whole dataset\n",
        "\n",
        "print(vocab_count)\n",
        "# Counter({'the': 47430, 'to': 35684, 'and': 26245, 'of': 24176, 'a': 19290, 'in': 17442, 'you': 14258, ...\n",
        "\n",
        "vocab_count = vocab_count.most_common(len(vocab_count))\n",
        "\n",
        "vocab_to_int = {word : index+2 for index, (word, count) in enumerate(vocab_count)}\n",
        "vocab_to_int.update({'__PADDING__': 0}) # index 0 for padding\n",
        "vocab_to_int.update({'__UNKNOWN__': 1}) # index 1 for unknown word such as broken character\n",
        "\n",
        "print(vocab_to_int)\n",
        "# {'the': 2, 'to': 3, 'and': 4, 'of': 5, 'a': 6, 'in': 7, 'you': 8, 'for': 9, \"'\": 10, 'is': 11, ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ8bboqcgJSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Tokenize & Vectorize sequences\n",
        "vectorized_seqs = []\n",
        "for seq in data: \n",
        "  vectorized_seqs.append([vocab_to_int[word] for word in seq.split()])\n",
        "\n",
        "# Save the lengths of sequences\n",
        "seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
        "\n",
        "# Add padding(0)\n",
        "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "  seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "  \n",
        "\n",
        "print(seq_lengths.max()) # tensor(30772)\n",
        "print(seq_tensor[0]) # tensor([ 20,  77, 666,  ...,   0,   0,   0])\n",
        "print(seq_lengths[0]) # tensor(412)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Ma75fIiLeS",
        "colab_type": "code",
        "outputId": "5e84e2c2-1757-4f68-d7da-304c3a646ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "sample = \"operations is digging out 2000 feet of pipe to begin the hydro test\"\n",
        "\n",
        "tokenized_sample = [ word for word in sample.split()]\n",
        "print(tokenized_sample[:3]) # ['operations', 'is', 'digging']\n",
        "\n",
        "vectorized_sample = [ vocab_to_int.get(word, 1) for word in tokenized_sample] # unknown word in dict marked as 1\n",
        "print(vectorized_sample[:3]) # [424, 11, 14683]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['operations', 'is', 'digging']\n",
            "[424, 11, 14683]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc71-icLjIwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data.sampler as splr\n",
        "\n",
        "\n",
        "class CustomDataLoader(object):\n",
        "  def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_tensor = seq_tensor\n",
        "    self.seq_lengths = seq_lengths\n",
        "    self.label_tensor = label_tensor\n",
        "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
        "    self.sampler_iter = iter(self.sampler)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
        "    return self\n",
        "\n",
        "  def _next_index(self):\n",
        "    return next(self.sampler_iter) # may raise StopIteration\n",
        "\n",
        "  def __next__(self):\n",
        "    index = self._next_index()\n",
        "\n",
        "    subset_seq_tensor = self.seq_tensor[index]\n",
        "    subset_seq_lengths = self.seq_lengths[index]\n",
        "    subset_label_tensor = self.label_tensor[index]\n",
        "\n",
        "    # order by length to use pack_padded_sequence()\n",
        "    subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
        "    subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
        "    subset_label_tensor = subset_label_tensor[perm_idx]\n",
        "\n",
        "    return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sampler)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5mlBcf_hbhA",
        "colab_type": "code",
        "outputId": "20de31bb-aed6-417c-dfff-3059387b3386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "shuffled_idx = torch.randperm(label.shape[0])\n",
        "\n",
        "seq_tensor = seq_tensor[shuffled_idx]\n",
        "seq_lenghts = seq_lengths[shuffled_idx]\n",
        "label = label[shuffled_idx]\n",
        "\n",
        "PCT_TRAIN = 0.7\n",
        "PCT_VALID = 0.2\n",
        "\n",
        "length = len(label)\n",
        "train_seq_tensor = seq_tensor[:int(length*PCT_TRAIN)] \n",
        "train_seq_lengths = seq_lengths[:int(length*PCT_TRAIN)]\n",
        "train_label = label[:int(length*PCT_TRAIN)]\n",
        "\n",
        "valid_seq_tensor = seq_tensor[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_seq_lengths = seq_lengths[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_label = label[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
        "\n",
        "test_seq_tensor = seq_tensor[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "test_seq_lengths = seq_lengths[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "test_label = label[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "\n",
        "print(train_seq_tensor.shape) # torch.Size([4200, 30772])\n",
        "print(valid_seq_tensor.shape) # torch.Size([1199, 30772])\n",
        "print(test_seq_tensor.shape) # torch.Size([601, 30772])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4200, 30772])\n",
            "torch.Size([1199, 30772])\n",
            "torch.Size([601, 30772])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3JReWZStM0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set shuffle = False since data is already shuffled\n",
        "batch_size = 80\n",
        "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
        "valid_loader = CustomDataLoader(valid_seq_tensor, valid_seq_lengths, valid_label, batch_size)\n",
        "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEUW3ETgu6ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class SpamHamLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers,\\\n",
        "                 drop_lstm=0.1, drop_out = 0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_lstm, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "\n",
        "        # embeddings\n",
        "        embedded_seq_tensor = self.embedding(x)\n",
        "                \n",
        "        # pack, remove pads\n",
        "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
        "        \n",
        "        # lstm\n",
        "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
        "          # https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html\n",
        "          # If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero\n",
        "\n",
        "        # unpack, recover padded sequence\n",
        "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "       \n",
        "        # collect the last output in each batch\n",
        "        last_idxs = (input_sizes - 1).to(device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
        "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output).squeeze()\n",
        "               \n",
        "        # sigmoid function\n",
        "        output = self.sig(output)\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OBmZsymvL3_",
        "colab_type": "code",
        "outputId": "67f25104-1ea4-4198-f3bb-583069dc5121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "\n",
        "vocab_size = len(vocab_to_int)\n",
        "embedding_dim = 100 # int(vocab_size ** 0.25) # 15\n",
        "hidden_dim = 15\n",
        "output_size = 1\n",
        "n_layers = 2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "net = SpamHamLSTM(vocab_size, embedding_dim, hidden_dim, output_size, n_layers, \\\n",
        "                 0.2, 0.2)\n",
        "net = net.to(device)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpamHamLSTM(\n",
            "  (embedding): Embedding(67822, 100)\n",
            "  (lstm): LSTM(100, 15, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2)\n",
            "  (fc): Linear(in_features=15, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDqRXRCs0cCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "lr=0.03\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\\\n",
        "                                                       mode = 'min', \\\n",
        "                                                      factor = 0.5,\\\n",
        "                                                      patience = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQwJdYMy0gdB",
        "colab_type": "code",
        "outputId": "24480f6c-25fb-4de7-f4b8-6fc75dc3651e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 6 \n",
        "\n",
        "counter = 0\n",
        "print_every = 10\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "val_losses = []\n",
        "for e in range(epochs):\n",
        "  \n",
        "    scheduler.step(e)\n",
        "\n",
        "    for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "        counter += 1\n",
        "               \n",
        "        seq_tensor = seq_tensor.to(device)\n",
        "        seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "        label = label.to(device)\n",
        " \n",
        "        # get the output from the model\n",
        "        output = net(seq_tensor, seq_tensor_lengths)\n",
        "    \n",
        "        # get the loss and backprop\n",
        "        loss = criterion(output, label.float())\n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()\n",
        "        \n",
        "        # prevent the exploding gradient\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            \n",
        "            val_losses_in_itr = []\n",
        "            sums = []\n",
        "            sizes = []\n",
        "            \n",
        "            net.eval()\n",
        "            \n",
        "            for seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
        "\n",
        "                seq_tensor = seq_tensor.to(device)\n",
        "                seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "                label = label.to(device)\n",
        "                output = net(seq_tensor, seq_tensor_lengths)\n",
        "                \n",
        "                # losses\n",
        "                val_loss = criterion(output, label.float())     \n",
        "                val_losses_in_itr.append(val_loss.item())\n",
        "                \n",
        "                # accuracy\n",
        "                binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "                right_or_not = torch.eq(binary_output, label)\n",
        "                sums.append(torch.sum(right_or_not).float().item())\n",
        "                sizes.append(right_or_not.shape[0])\n",
        "            \n",
        "            accuracy = sum(sums) / sum(sizes)\n",
        "            \n",
        "            net.train()\n",
        "            print(\"Epoch: {:2d}/{:2d}\\t\".format(e+1, epochs),\n",
        "                  \"Steps: {:3d}\\t\".format(counter),\n",
        "                  \"Loss: {:.6f}\\t\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\\t\".format(np.mean(val_losses_in_itr)),\n",
        "                  \"Accuracy: {:.3f}\".format(accuracy))\n",
        "            \n",
        "# Epoch:  1/ 6\t Steps:  10\t Loss: 0.693371\t Val Loss: 0.689860\t Accuracy: 0.530\n",
        "# Epoch:  1/ 6\t Steps:  20\t Loss: 0.699150\t Val Loss: 0.667903\t Accuracy: 0.585\n",
        "# Epoch:  1/ 6\t Steps:  30\t Loss: 0.631709\t Val Loss: 0.626028\t Accuracy: 0.651\n",
        "# Epoch:  1/ 6\t Steps:  40\t Loss: 0.609348\t Val Loss: 0.538908\t Accuracy: 0.716\n",
        "# Epoch:  1/ 6\t Steps:  50\t Loss: 0.435395\t Val Loss: 0.440515\t Accuracy: 0.780\n",
        "# Epoch:  2/ 6\t Steps:  60\t Loss: 0.364830\t Val Loss: 0.312334\t Accuracy: 0.892\n",
        "# Epoch:  2/ 6\t Steps:  70\t Loss: 0.177650\t Val Loss: 0.283867\t Accuracy: 0.901\n",
        "# Epoch:  2/ 6\t Steps:  80\t Loss: 0.379663\t Val Loss: 0.360904\t Accuracy: 0.883\n",
        "# Epoch:  2/ 6\t Steps:  90\t Loss: 0.399583\t Val Loss: 0.390520\t Accuracy: 0.857\n",
        "# Epoch:  2/ 6\t Steps: 100\t Loss: 0.467552\t Val Loss: 0.480415\t Accuracy: 0.808\n",
        "# Epoch:  3/ 6\t Steps: 110\t Loss: 0.239100\t Val Loss: 0.282348\t Accuracy: 0.896\n",
        "# Epoch:  3/ 6\t Steps: 120\t Loss: 0.091864\t Val Loss: 0.252968\t Accuracy: 0.915\n",
        "# Epoch:  3/ 6\t Steps: 130\t Loss: 0.160094\t Val Loss: 0.209478\t Accuracy: 0.934     \n",
        "# I halted the training process at step 130"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1/ 6\t Steps:  10\t Loss: 0.693371\t Val Loss: 0.689860\t Accuracy: 0.530\n",
            "Epoch:  1/ 6\t Steps:  20\t Loss: 0.699150\t Val Loss: 0.667903\t Accuracy: 0.585\n",
            "Epoch:  1/ 6\t Steps:  30\t Loss: 0.631709\t Val Loss: 0.626028\t Accuracy: 0.651\n",
            "Epoch:  1/ 6\t Steps:  40\t Loss: 0.609348\t Val Loss: 0.538908\t Accuracy: 0.716\n",
            "Epoch:  1/ 6\t Steps:  50\t Loss: 0.435395\t Val Loss: 0.440515\t Accuracy: 0.780\n",
            "Epoch:  2/ 6\t Steps:  60\t Loss: 0.364830\t Val Loss: 0.312334\t Accuracy: 0.892\n",
            "Epoch:  2/ 6\t Steps:  70\t Loss: 0.177650\t Val Loss: 0.283867\t Accuracy: 0.901\n",
            "Epoch:  2/ 6\t Steps:  80\t Loss: 0.379663\t Val Loss: 0.360904\t Accuracy: 0.883\n",
            "Epoch:  2/ 6\t Steps:  90\t Loss: 0.399583\t Val Loss: 0.390520\t Accuracy: 0.857\n",
            "Epoch:  2/ 6\t Steps: 100\t Loss: 0.467552\t Val Loss: 0.480415\t Accuracy: 0.808\n",
            "Epoch:  3/ 6\t Steps: 110\t Loss: 0.239100\t Val Loss: 0.282348\t Accuracy: 0.896\n",
            "Epoch:  3/ 6\t Steps: 120\t Loss: 0.091864\t Val Loss: 0.252968\t Accuracy: 0.915\n",
            "Epoch:  3/ 6\t Steps: 130\t Loss: 0.160094\t Val Loss: 0.209478\t Accuracy: 0.934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-83b5050649b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# prevent the exploding gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2b315Bh3QcR",
        "colab_type": "code",
        "outputId": "f3cce04e-1af5-46e1-82ac-c26ac33e0997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_losses = []\n",
        "sums = []\n",
        "sizes = []\n",
        "\n",
        "net.eval()\n",
        "\n",
        "test_losses = []\n",
        "for seq_tensor, seq_tensor_lengths, label in iter(test_loader):\n",
        "\n",
        "    seq_tensor = seq_tensor.to(device)\n",
        "    seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "    label = label.to(device)\n",
        "    output = net(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "    # losses\n",
        "    test_loss = criterion(output, label.float())     \n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # accuracy\n",
        "    binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "    right_or_not = torch.eq(binary_output, label)\n",
        "    sums.append(torch.sum(right_or_not).float().item())\n",
        "    sizes.append(right_or_not.shape[0])\n",
        "\n",
        "accuracy = np.sum(sums) / np.sum(sizes)\n",
        "print(\"Test Loss: {:.6f}\\t\".format(np.mean(test_losses)),\n",
        "      \"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.208418\t Accuracy: 0.927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsSYt9N73bzo",
        "colab_type": "code",
        "outputId": "25ca2a36-2083-42a1-e1d3-fa21fc0f6adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "net.eval()\n",
        "myString = \"Have you been really busy this week? \\\n",
        "  Then you'll definitely want to make time for this lesson. \\\n",
        "  Have a wonderful week, learn something new, and practice some English!\"\n",
        "\n",
        "\n",
        "# get rid of some characters\n",
        "unnecessary =  [\"-\", \".\", \",\", \"/\", \":\", \"@\", \"'\", \"!\"]\n",
        "content = myString.lower()\n",
        "content = ''.join([c for c in content if c not in unnecessary])\n",
        "input = [content]\n",
        "\n",
        "# Tokenize & Vectorize sequences\n",
        "vectorized_seqs = []\n",
        "for seq in input: \n",
        "  vectorized_seqs.append([vocab_to_int.get(word,1) for word in seq.split()])\n",
        "   \n",
        "# Save the lengths of sequences\n",
        "seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
        "\n",
        "# Add padding(0)\n",
        "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "  seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "# Predict\n",
        "seq_tensor = seq_tensor.to(device)\n",
        "seq_lengths = seq_lengths.to(device)\n",
        "output = net(seq_tensor, seq_lengths)\n",
        "\n",
        "print(output.item()) \n",
        "# 0.64 (>0.5), means SPAM (actually, it is a part of the advertisement of English lesson)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6431859135627747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk35OQbRFqv-",
        "colab_type": "code",
        "outputId": "b1c2b7f3-54bd-4078-d475-1acf2882e57d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "seqs = torch.tensor([[1,2,3,4,5], [6,7,8,0,0]])\n",
        "lengths = torch.tensor([5,3], dtype = torch.int64).cpu()  # should be a 1D / CPU / int64 tensor\n",
        "result = pack_padded_sequence(seqs, lengths, batch_first=True)\n",
        "print(result.data) # tensor([1, 6, 2, 7, 3, 8, 4, 5])\n",
        "print(result.batch_sizes) # tensor([2, 2, 2, 1, 1])\n",
        "\n",
        "# seq_1) 1 2 3 4 5\n",
        "# seq_2) 6 7 8 0 0\n",
        "# batch) 2 2 2 1 1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 6, 2, 7, 3, 8, 4, 5])\n",
            "tensor([2, 2, 2, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}